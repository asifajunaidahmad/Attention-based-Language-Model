# -*- coding: utf-8 -*-
"""GPT2Model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16f13zwBisW82US-grBdm8jldVInxtzFp
"""

from google.colab import drive
drive.mount('/content/drive')

"""# **Transformers**
 Fot this exam I am using transformer, which is a deep learning model that adopts the mechanism of self-attention, differentially weighting the significance of each part of the input data.
"""

# Installing Transformers
!pip install transformers

"""**Importing Libraries**"""

import pandas as pd
from torch.utils.data import Dataset
from transformers import GPT2Tokenizer, TrainingArguments, Trainer, GPT2LMHeadModel, DataCollatorForLanguageModeling,Trainer,IntervalStrategy

"""**Loading Dataset**"""

df = pd.read_csv('/content/drive/MyDrive/notes-train.csv',sep='\t', lineterminator='\r',skipinitialspace=True)

"""# Preprocessing of Data"""

# checking the dataset
df.shape

#checking the contents of the data
df.head(2)

# Column Heading Text has spaces
df.rename(columns = {'text,,,,,,,':'text'}, inplace = True)

df.text[0]

"""The purpose of below function is to remove noise from the text like dates especially anything between brackets[], new line character and reducing non alphbet characters"""

import regex as re
from numpy import character
clean_text =""
df.text = df.text.apply(lambda x: re.sub("(\[.*?]\))", "", x))

for row in df.text:
  coulmns = re.split('[a-z A-Z]*:', row)
  for col in coulmns:
    col = col.strip()
    col = re.sub("[\n]+","",col)
    if len(col) > 100:
      number_of_characters = len(re.findall('\w',col))
      non_characters = re.findall('\W', col)
      whitespace = non_characters.count(' ')
      number_of_non_characters = len(non_characters) - whitespace
      if number_of_non_characters ==0:
        clean_text = clean_text +col+ '. '
      elif number_of_characters/number_of_non_characters >5:
        clean_text = clean_text +col+ '. '
clean_sentence = clean_text.split('.')
clean_sentence =[x.strip() for x  in clean_sentence if len(x) >20]

clean_sentence[:2]

"""No additional preprocessing required for problem in hand, moreover, no tokeninzation required because everything is includes in the model i am going to use

**Splitting Data for Training & Testing**
"""

from sklearn.model_selection import train_test_split
train, test = train_test_split(df.text.values, test_size = 0.2)

"""**Loading Pretrained Language Model GPTMedium**

I used GPT-2  Model for this problem because it is a language model used to generate the next text from given text. GPT-2 is a large transformer-based language model with 1.5 billion parameters, trained on a dataset of 8 million web pages. GPT-2 is trained with a simple objective: predict the next word, given all of the previous words within some text. The diversity of the dataset causes this simple goal to contain naturally occurring demonstrations of many tasks across diverse domains. GPT-2 is a direct scale-up of GPT, with more than 10X the parameters and trained on more than 10X the amount of data.

**Hugging Face Transformers**

(Hugging Face) Transformers (formerly known as PyTorch-transformers and PyTorch-pretrained-bert) provides general-purpose architectures (BERT, GPT-2, RoBERTa, XLM, DistilBert, XLNetâ€¦) for Natural Language Understanding (NLU) and Natural Language Generation (NLG) with over 32+ pre-trained models in 100+ languages and deep interoperability between TensorFlow 2.0 and PyTorch.

Basically Hugging Face Transformers is the mega python package that has some pre-defined or pre-trained functions, pipelines, and models. which we can use for our natural language processing tasks.

GPT-2 Tokenizer and Models are also included in Transformers.
"""

#Loading a pretrained language model

tokenizer = GPT2Tokenizer.from_pretrained('distilgpt2', bos_token='<|startoftext|>',
                                          eos_token='<|endoftext|>', pad_token='<|pad|>')
model = GPT2LMHeadModel.from_pretrained('distilgpt2')
model.resize_token_embeddings(len(tokenizer))

#Tokenizing train and test text data
train_encodings = tokenizer([*train], truncation=True, padding=True,max_length=512)
test_encodings = tokenizer([*test], truncation=True, padding=True,max_length=512)

"""The model is trained using Huggingface through PyTorch"""

import torch
class MyDataset(torch.utils.data.Dataset):
    def __init__(self, encodings):
        self.encodings = encodings
        # self.labels = labels

    def __getitem__(self, idx):
        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}
        # if self.labels:
        #     item["labels"] = torch.tensor(self.labels[idx])
        return item

    def __len__(self):
        return len(self.encodings["input_ids"])

# Dataset class provides extra functioality like batches that is used by training loop 
train_dataset = MyDataset(train_encodings)
test_dataset = MyDataset(test_encodings)

# Data collator does processing like masking that is used by language models
data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)

# training arguments to use
training_args = TrainingArguments(
    output_dir="./output",
    overwrite_output_dir=True, 
    num_train_epochs=.5, 
    per_device_train_batch_size=8, 
    per_device_eval_batch_size=32, 
    eval_steps = 100,
    save_strategy = IntervalStrategy.STEPS,
    evaluation_strategy = IntervalStrategy.STEPS,
    save_steps=8000, 
    warmup_steps=500,
    #prediction_loss_only=True,
    fp16 =True,
    load_best_model_at_end=True
    )

# training loop
trainer = Trainer(
    model=model,
    args=training_args,
    data_collator=data_collator,
    train_dataset=train_dataset,
    eval_dataset=test_dataset,
)

trainer.train()

"""# ROGUE SCORE"""

!pip install rouge

# generating text to assess our models performance 
from transformers import pipeline
model = model.to('cpu')
generate = pipeline('text-generation',model=model, tokenizer=tokenizer,config={'max_length':800})

# calculating ROGUE score , percsion and recall on a sample text
# passing a sample text from the data to see how model regenrate the text
from rouge import Rouge 

hypothesis =generate('As we discussed if you notice fever')[0]['generated_text']

reference = "As we discussed if you notice fever, worsening\rbreathing problems, or any other concerning symptoms to return\rto the emergency room immediately,START levofloxacin 750mg by mouth daily for 4 more days\r(\r\rYou should discuss with Dr"

rouge = Rouge()
scores = rouge.get_scores(hypothesis, reference)
scores

# example of regenerated text by model
print(hypothesis)